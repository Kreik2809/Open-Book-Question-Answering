import os
import torch

from transformers import TrainingArguments
from trl import DPOTrainer
from utils.validation_utils import is_similar

def preprocess_squad_dpo_samples(examples, model, tokenizer, device):
    """ This function is used to preprocess the samples for the DPO training.
    DPO dataset are composed of 3 sequences: the prompt, the chosen answer and the rejected answer.
    The prompt is the concatenation of the question and the context.
    The chosen answer is the ground truth answer if it exists, otherwise it is the predefined no answer sequence.
    The rejected answer is the first answer generated by the model that is not the no answer sequence or the no answer sequence.
    """
    no_answer_label = "The answer is not in the context."
    tokenized_no_answer = tokenizer(no_answer_label)["input_ids"]
    tokenizer.no_answer_token_ids = tokenized_no_answer + [tokenizer.eos_token_id]

    questions = [q.strip() for q in examples["question"]]
    contexts = examples["context"]
    prompts = [c + q for q, c in zip(questions, contexts)]
    no_answer_sequence = "The answer is not in the context." + tokenizer.decode(tokenizer.eos_token_id)
    
    answers = [a["text"][0].strip() + tokenizer.decode(tokenizer.eos_token_id) if len(a["answer_start"]) > 0 else generate_answer_from_prompt(prompts[i], model, tokenizer, device, no_answer_sequence) for i, a in enumerate(examples["answers"])]
    prefixes = ["The answer is :", "Sure! Here is the answer to your question :", "According to my knowledge, the answer is :"]
    answers = [prefixes[i % 3] + a for i, a in enumerate(answers)]
    
    inputs = {}
    inputs["prompt"] = prompts
    inputs["chosen"] = [answers[i] if len(a["answer_start"]) > 0 else no_answer_sequence  for i, a in enumerate(examples["answers"])]
    inputs["rejected"] = [answers[i] if len(a["answer_start"]) == 0 else no_answer_sequence  for i, a in enumerate(examples["answers"])]
    return inputs

def generate_answer_from_prompt(prompt, model, tokenizer, device, no_answer_sequence):
    """ This function is used to generate an answer from a prompt. It is used in the DPO training dataset creation. The answer chosen is the first one that is not the no answer sequence.
    """
    no_answer_token_ids = tokenizer(no_answer_sequence)["input_ids"]
    with torch.no_grad():
        prompt = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

        outputs = model.generate(prompt, do_sample=False,
                                            num_beams=20, 
                                            max_new_tokens=30, 
                                            num_return_sequences=10, 
                                            eos_token_id=tokenizer.eos_token_id, 
                                            pad_token_id=tokenizer.pad_token_id,
                                            return_dict_in_generate=True)
    
    selected_sequence = outputs.sequences[0]
    if is_similar(selected_sequence[prompt.shape[1]:], no_answer_token_ids, tokenizer):
        for i, sequence in enumerate(outputs.sequences):
            if not is_similar(sequence[prompt.shape[1]:], no_answer_token_ids, tokenizer):
                selected_sequence = sequence
                break 
    
    selected_sequence = tokenizer.decode(selected_sequence[prompt.shape[1]:], skip_special_tokens=True) + tokenizer.decode(tokenizer.eos_token_id) 
    return selected_sequence


def dpo_finetuning(model, tokenizer, dpo_train_dataset, output_dir, max_steps, epochs, lr, beta, batch_size, logger, fp16=False, optim="adamw_torch"):
    """ This function is used to finetune a model using DPO.
    """
    args = TrainingArguments(
            per_device_train_batch_size=batch_size,
            max_steps=max_steps,
            num_train_epochs=epochs,
            learning_rate=lr,
            logging_steps=5,
            output_dir=output_dir,
            save_strategy = "steps",
            save_steps = 2500,
            fp16=fp16,
            optim=optim,
        )

    trainer = DPOTrainer(
            model,
            args=args,
            beta=beta,
            train_dataset=dpo_train_dataset,
            tokenizer=tokenizer,
        )

    train_results = trainer.train()
    metrics = train_results.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()
    logger.info(metrics)                 
    # Saving model
    logger.info("Saving last checkpoint of the model...")
    trainer.model.save_pretrained(os.path.join(output_dir, "last_checkpoint"))
    tokenizer.save_pretrained(os.path.join(output_dir, "last_checkpoint"))
    
    # Free memory 
    del model
    del trainer