{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "        context = filedata.split('\\n')\n",
    "        context = ''.join(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinch_node_red = load_context(\"mmd/sinch_doc_node_red.mmd\")\n",
    "sinch_webhook = load_context(\"mmd/sinch_doc_how_to_webhook.mmd\")\n",
    "sinch_overview = load_context(\"mmd/sinch_doc_overview.mmd\")\n",
    "nougat_context = load_context(\"mmd/nougat.mmd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicsou/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"../../extractive_qa/squad_experiments/code/outputs/squad2_microsoft/deberta-v3-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\")\n",
    "\n",
    "    input_ids = inputs.pop(\"input_ids\") #Token ids\n",
    "    offsets = inputs.pop(\"offset_mapping\") #Offsets to adjust token id to the original text\n",
    "    attention_mask = inputs.pop(\"attention_mask\") #Mask to not consider pad tokens as input\n",
    "   \n",
    "    n_best = 10\n",
    "\n",
    "    answers = []\n",
    "    for i in range(len(input_ids)):\n",
    "        input_id = torch.tensor(input_ids[i]).unsqueeze(0)\n",
    "        attention_mask_ = torch.tensor(attention_mask[i]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_id, attention_mask=attention_mask_)\n",
    "            \n",
    "        start_logits = outputs.start_logits.cpu().detach().numpy()[0]\n",
    "        end_logits = outputs.end_logits.cpu().detach().numpy()[0]\n",
    "\n",
    "        offset = offsets[i]\n",
    "        sequence_ids = inputs.sequence_ids(i) #The ids of the current sequence tokens, 1 for the context tokens and 0 for the question tokens\n",
    "        offset = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "        start_indexes = np.argsort(start_logits)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logits)[-1 : -n_best - 1 : -1].tolist()\n",
    "\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                if ( end_index < start_index) or (end_index - start_index + 1 > 30):\n",
    "                    continue\n",
    "                answer = {\n",
    "                    \"text\": context[offset[start_index][0] : offset[end_index][1]], #The answer is the text between the start and end index\n",
    "                    \"logit_score\": start_logits[start_index] + end_logits[end_index], #The score is the sum of the start and end logit\n",
    "                    \"no_answer_probability\": (start_logits[0] + end_logits[0]) - (start_logits[start_index] + end_logits[end_index]), #The score of the null answer is the sum of the start and end logit for the first token\n",
    "                }\n",
    "                if answer[\"no_answer_probability\"] < -2:\n",
    "                    answers.append(answer)\n",
    "\n",
    "    if len(answers) > 0:\n",
    "        #Return the answer with the highest logit score\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    else:\n",
    "        #No answer found\n",
    "        best_answer = {\"text\": \"No answer found\", \"logit_score\": 0.0, \"no_answer_probability\": 0.0}\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinch doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['question', 'answer', 'logit_score', 'no_answer_probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/bb3csk5s38sbjv27nsbhyy9m0000gp/T/ipykernel_33046/3618118971.py:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Node RED ? \"\n",
    "context = sinch_node_red\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"In few words, What is Node RED ? \"\n",
    "context = sinch_node_red\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the supported channels of Node RED ? \"\n",
    "context = sinch_node_red\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"In which cases can I use Node RED ? \"\n",
    "context = sinch_node_red\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the differents nodes of Sinch Messaging ? \"\n",
    "context = sinch_node_red\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was Node RED released ? \"\n",
    "context = sinch_node_red\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me the different steps to add a webhook to my app ? \"\n",
    "context = sinch_webhook\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the Sinch Conversation API ?\"\n",
    "context = sinch_overview\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use the Sinch Conversation API with Viber Business ? \"\n",
    "context = sinch_overview\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use the Sinch Conversation API with Outlook ? \"\n",
    "context = sinch_overview\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Where are the hosting locations for the Conversation API ? \"\n",
    "context = sinch_overview\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the specific pricing details for using the Sinch Conversation API ? \"\n",
    "context = sinch_overview\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does the Sinch Conversation API handle multimedia content like images and videos ? \"\n",
    "context = sinch_overview\n",
    "output = inference(question, context)\n",
    "df = pd.concat([df, pd.DataFrame([[question, output['text'], output['logit_score'], output['no_answer_probability']]], columns=['question', 'answer', 'logit_score', 'no_answer_probability'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['logit_score', 'no_answer_probability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the two last columns\n",
    "latex_code = df.to_latex(index=False, column_format=\"|p{5cm}|p{10cm}|\", float_format=(lambda x: \"%.3f\" % x))\n",
    "latex_code = latex_code.replace('\\\\toprule', '\\hline')\n",
    "latex_code = latex_code.replace('\\\\bottomrule', '\\hline')\n",
    "latex_code = latex_code.replace('\\\\\\n', '\\\\ \\hline\\n')\n",
    "\n",
    "with open('../outputs/extractive_qa.tex', 'w') as file:\n",
    "    file.write(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
